{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mT5 Question Answering System with LoRA Fine-tuning\n",
    "## Bangla NCTB Textbook Dataset\n",
    "\n",
    "This notebook fine-tunes mT5-base model using LoRA for Bangla question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers peft datasets sentencepiece accelerate matplotlib pandas torch scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration,\n",
    "    MT5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Set random seeds\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset - Update this path to your CSV file location\n",
    "df = pd.read_csv('Textbook_Dataset_from_NCTB.csv')\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few samples:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "print(f\"Samples after cleaning: {len(df)}\")\n",
    "\n",
    "# Analyze text lengths\n",
    "df['passage_length'] = df['Passage'].str.len()\n",
    "df['question_length'] = df['Question'].str.len()\n",
    "df['answer_length'] = df['AnsText'].str.len()\n",
    "\n",
    "print(f\"\\nText Statistics:\")\n",
    "print(df[['passage_length', 'question_length', 'answer_length']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset (80% train, 10% validation, 10% test)\n",
    "train_size = int(0.8 * len(df))\n",
    "val_size = int(0.1 * len(df))\n",
    "\n",
    "train_df = df[:train_size].reset_index(drop=True)\n",
    "val_df = df[train_size:train_size + val_size].reset_index(drop=True)\n",
    "test_df = df[train_size + val_size:].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Validation size: {len(val_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    \"\"\"Custom Dataset for Question Answering\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, tokenizer, max_source_length=512, max_target_length=128):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Format: \"question: <question> context: <passage>\"\n",
    "        source_text = f\"question: {row['Question']} context: {row['Passage']}\"\n",
    "        target_text = row['AnsText']\n",
    "        \n",
    "        # Tokenize source\n",
    "        source_encoding = self.tokenizer(\n",
    "            source_text,\n",
    "            max_length=self.max_source_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize target\n",
    "        target_encoding = self.tokenizer(\n",
    "            target_text,\n",
    "            max_length=self.max_target_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        labels = target_encoding['input_ids'].squeeze()\n",
    "        # Replace padding token id with -100 for loss computation\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            'input_ids': source_encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': source_encoding['attention_mask'].squeeze(),\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Normalize answer text for evaluation\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    \"\"\"Compute exact match score\"\"\"\n",
    "    return int(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    \"\"\"Compute F1 score between prediction and ground truth\"\"\"\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    truth_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = Counter(pred_tokens) & Counter(truth_tokens)\n",
    "    num_common = sum(common_tokens.values())\n",
    "    \n",
    "    if num_common == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(truth_tokens)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def compute_metrics(predictions, ground_truths):\n",
    "    \"\"\"Compute exact match and F1 scores for a list of predictions\"\"\"\n",
    "    em_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for pred, truth in zip(predictions, ground_truths):\n",
    "        em_scores.append(exact_match_score(pred, truth))\n",
    "        f1_scores.append(f1_score(pred, truth))\n",
    "    \n",
    "    return {\n",
    "        'exact_match': np.mean(em_scores) * 100,\n",
    "        'f1': np.mean(f1_scores) * 100\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Model and Apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "model_name = \"google/mt5-base\"\n",
    "print(f\"Loading tokenizer: {model_name}\")\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load base model\n",
    "print(f\"Loading model: {model_name}\")\n",
    "base_model = MT5ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in base_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,  # LoRA alpha parameter\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q\", \"v\"]  # Apply LoRA to query and value matrices\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = QADataset(train_df, tokenizer)\n",
    "val_dataset = QADataset(val_df, tokenizer)\n",
    "test_dataset = QADataset(test_df, tokenizer)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 5\n",
    "learning_rate = 5e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {int(0.1 * total_steps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, tokenizer, device):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Evaluating\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Compute loss\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            total_loss += outputs.loss.item()\n",
    "            \n",
    "            # Generate predictions\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            # Decode predictions and labels\n",
    "            pred_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            label_ids = labels.clone()\n",
    "            label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "            label_texts = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "            \n",
    "            predictions.extend(pred_texts)\n",
    "            ground_truths.extend(label_texts)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    metrics = compute_metrics(predictions, ground_truths)\n",
    "    \n",
    "    return avg_loss, metrics, predictions, ground_truths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_em_scores = []\n",
    "val_f1_scores = []\n",
    "best_f1 = 0\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    train_losses.append(train_loss)\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_metrics, _, _ = evaluate(model, val_loader, tokenizer, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_em_scores.append(val_metrics['exact_match'])\n",
    "    val_f1_scores.append(val_metrics['f1'])\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Exact Match: {val_metrics['exact_match']:.2f}%\")\n",
    "    print(f\"F1 Score: {val_metrics['f1']:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['f1'] > best_f1:\n",
    "        best_f1 = val_metrics['f1']\n",
    "        print(f\"âœ“ New best F1 score! Saving model...\")\n",
    "        model.save_pretrained('./best_model')\n",
    "        tokenizer.save_pretrained('./best_model')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training Completed!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "print(\"Loading best model...\")\n",
    "best_model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "best_model = PeftModel.from_pretrained(best_model, './best_model')\n",
    "best_model = best_model.to(device)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_metrics, test_predictions, test_ground_truths = evaluate(\n",
    "    best_model, test_loader, tokenizer, device\n",
    ")\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Exact Match: {test_metrics['exact_match']:.2f}%\")\n",
    "print(f\"F1 Score: {test_metrics['f1']:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "epochs_range = range(1, num_epochs + 1)\n",
    "\n",
    "# Plot 1: Training and Validation Loss\n",
    "axes[0, 0].plot(epochs_range, train_losses, 'b-o', label='Training Loss', linewidth=2)\n",
    "axes[0, 0].plot(epochs_range, val_losses, 'r-o', label='Validation Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 0].set_title('Training and Validation Loss per Epoch', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Exact Match Score\n",
    "axes[0, 1].plot(epochs_range, val_em_scores, 'g-o', label='Exact Match', linewidth=2, markersize=8)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Exact Match (%)', fontsize=12)\n",
    "axes[0, 1].set_title('Exact Match Score per Epoch', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_ylim([0, 100])\n",
    "\n",
    "# Plot 3: F1 Score\n",
    "axes[1, 0].plot(epochs_range, val_f1_scores, 'm-o', label='F1 Score', linewidth=2, markersize=8)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('F1 Score (%)', fontsize=12)\n",
    "axes[1, 0].set_title('F1 Score per Epoch', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_ylim([0, 100])\n",
    "\n",
    "# Plot 4: Combined Metrics\n",
    "axes[1, 1].plot(epochs_range, val_em_scores, 'g-o', label='Exact Match', linewidth=2, markersize=8)\n",
    "axes[1, 1].plot(epochs_range, val_f1_scores, 'm-o', label='F1 Score', linewidth=2, markersize=8)\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Score (%)', fontsize=12)\n",
    "axes[1, 1].set_title('Exact Match vs F1 Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training metrics saved to 'training_metrics.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training results\n",
    "results = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'val_em_scores': val_em_scores,\n",
    "    'val_f1_scores': val_f1_scores,\n",
    "    'test_loss': test_loss,\n",
    "    'test_exact_match': test_metrics['exact_match'],\n",
    "    'test_f1': test_metrics['f1'],\n",
    "    'num_epochs': num_epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'learning_rate': learning_rate\n",
    "}\n",
    "\n",
    "with open('training_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"Training results saved to 'training_results.json'\")\n",
    "\n",
    "# Save test predictions\n",
    "test_results_df = pd.DataFrame({\n",
    "    'question': test_df['Question'].values,\n",
    "    'context': test_df['Passage'].values,\n",
    "    'ground_truth': test_ground_truths,\n",
    "    'prediction': test_predictions\n",
    "})\n",
    "test_results_df.to_csv('test_predictions.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Test predictions saved to 'test_predictions.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Show Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample predictions\n",
    "print(\"\\nSample Predictions from Test Set:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "num_samples = min(10, len(test_results_df))\n",
    "sample_indices = np.random.choice(len(test_results_df), num_samples, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    row = test_results_df.iloc[idx]\n",
    "    print(f\"\\nQuestion: {row['question']}\")\n",
    "    print(f\"Ground Truth: {row['ground_truth']}\")\n",
    "    print(f\"Prediction: {row['prediction']}\")\n",
    "    \n",
    "    # Calculate match\n",
    "    em = exact_match_score(row['prediction'], row['ground_truth'])\n",
    "    f1 = f1_score(row['prediction'], row['ground_truth'])\n",
    "    print(f\"EM: {em}, F1: {f1:.2f}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Test Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, context, model, tokenizer, device):\n",
    "    \"\"\"Generate answer for a given question and context\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Format input\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    \n",
    "    # Tokenize\n",
    "    input_ids = tokenizer(\n",
    "        input_text,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).input_ids.to(device)\n",
    "    \n",
    "    # Generate answer\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # Decode answer\n",
    "    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the inference function\n",
    "sample = test_df.iloc[0]\n",
    "\n",
    "print(\"Test Inference:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Question: {sample['Question']}\")\n",
    "print(f\"Context: {sample['Passage'][:200]}...\")\n",
    "print(f\"\\nGround Truth: {sample['AnsText']}\")\n",
    "\n",
    "predicted_answer = answer_question(\n",
    "    sample['Question'], \n",
    "    sample['Passage'], \n",
    "    best_model, \n",
    "    tokenizer, \n",
    "    device\n",
    ")\n",
    "print(f\"Prediction: {predicted_answer}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Print Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Total samples: {len(df)}\")\n",
    "print(f\"  Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"\\nInitial Performance (Epoch 1):\")\n",
    "print(f\"  Train Loss: {train_losses[0]:.4f}\")\n",
    "print(f\"  Val Loss: {val_losses[0]:.4f}\")\n",
    "print(f\"  Exact Match: {val_em_scores[0]:.2f}%\")\n",
    "print(f\"  F1 Score: {val_f1_scores[0]:.2f}%\")\n",
    "print(f\"\\nFinal Performance (Epoch {num_epochs}):\")\n",
    "print(f\"  Train Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"  Val Loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"  Exact Match: {val_em_scores[-1]:.2f}%\")\n",
    "print(f\"  F1 Score: {val_f1_scores[-1]:.2f}%\")\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  Exact Match: {test_metrics['exact_match']:.2f}%\")\n",
    "print(f\"  F1 Score: {test_metrics['f1']:.2f}%\")\n",
    "print(f\"\\nImprovement:\")\n",
    "print(f\"  Loss reduction: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.1f}%\")\n",
    "print(f\"  EM improvement: +{val_em_scores[-1] - val_em_scores[0]:.1f}%\")\n",
    "print(f\"  F1 improvement: +{val_f1_scores[-1] - val_f1_scores[0]:.1f}%\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training completed successfully!\")\n",
    "print(\"Model saved in './best_model' directory\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
